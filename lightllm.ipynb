{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Required Libraries\n",
    "Use pip to install Flask, LightLLM, and Google Cloud SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Install required libraries\n",
    "!pip install transformers lightllm google-cloud-build\n",
    "\n",
    "# Authenticate with Google Cloud\n",
    "!gcloud auth login\n",
    "\n",
    "# Set Google Cloud project\n",
    "!gcloud config set project YOUR_PROJECT_ID\n",
    "\n",
    "# Set up Hugging Face authentication\n",
    "\n",
    "# Replace 'YOUR_HUGGING_FACE_TOKEN' with your actual Hugging Face token\n",
    "login(token='YOUR_HUGGING_FACE_TOKEN')\n",
    "\n",
    "# Import required libraries\n",
    "\n",
    "# Download model and tokenizer from Hugging Face\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, use_auth_token=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "# Save the model and tokenizer locally\n",
    "model.save_pretrained(\"./model\")\n",
    "tokenizer.save_pretrained(\"./model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Flask Application\n",
    "Create a Flask application that uses LightLLM to generate responses based on a given prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "from flask import Flask, request, jsonify\n",
    "from lightllm import LLMPredictor\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Initialize Flask Application\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "# Load LightLLM Model\n",
    "predictor = LLMPredictor(model_path=\"./model\")\n",
    "\n",
    "# Define Route for Text Prediction\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    # Get JSON data from the POST request\n",
    "    data = request.get_json()\n",
    "    \n",
    "    # Extract the text input from the JSON data\n",
    "    text = data.get('text')\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    \n",
    "    # Perform inference using the model\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # Apply softmax to get probabilities\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    # Return the predictions as a JSON response\n",
    "    return jsonify(predictions.tolist())\n",
    "\n",
    "# Example usage\n",
    "text = \"Hello, how are you?\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "# Run Flask Application\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=8080)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dockerfile\n",
    "Write a Dockerfile to containerize the Flask application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dockerfile\n",
    "%%writefile Dockerfile\n",
    "# Use the official Python image from the Docker Hub\n",
    "FROM python:3.8-slim\n",
    "\n",
    "# Set the working directory in the container\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy the current directory contents into the container at /app\n",
    "COPY . /app\n",
    "\n",
    "# Install any needed packages specified in requirements.txt\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Make port 8080 available to the world outside this container\n",
    "EXPOSE 8080\n",
    "\n",
    "# Define environment variable\n",
    "ENV NAME World\n",
    "\n",
    "# Run app.py when the container launches\n",
    "CMD [\"python\", \"app.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Push Docker Image\n",
    "Build the Docker image and push it to Google Container Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with Google Cloud\n",
    "!gcloud auth login\n",
    "\n",
    "# Set Google Cloud project\n",
    "!gcloud config set project YOUR_PROJECT_ID\n",
    "\n",
    "# Create a cloudbuild.yaml file\n",
    "cloudbuild_yaml = \"\"\"\n",
    "steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', 'gcr.io/YOUR_PROJECT_ID/lightllm-app', '.']\n",
    "images:\n",
    "- 'gcr.io/YOUR_PROJECT_ID/lightllm-app'\n",
    "\"\"\"\n",
    "with open('cloudbuild.yaml', 'w') as file:\n",
    "    file.write(cloudbuild_yaml)\n",
    "\n",
    "# Submit build to Google Cloud Build\n",
    "!gcloud builds submit --config cloudbuild.yaml ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy to Cloud Run\n",
    "Deploy the Docker image to Cloud Run using the gcloud command-line tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "!gcloud run deploy distilbert-app --image gcr.io/YOUR_PROJECT_ID/lightllm-app --platform managed --region us-central1 --allow-unauthenticated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Deployed Service\n",
    "Send a request to the deployed Cloud Run service to test the LightLLM integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Deployed Service\n",
    "import requests\n",
    "\n",
    "# Replace with your Cloud Run service URL\n",
    "cloud_run_url = \"https://YOUR_CLOUD_RUN_SERVICE_URL\"\n",
    "\n",
    "# Define the prompt to send to the service\n",
    "prompt = \"Hello, how are you?\"\n",
    "\n",
    "# Send a POST request to the deployed service\n",
    "response = requests.post(\n",
    "    f\"{cloud_run_url}/predict\",\n",
    "    json={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# Print the response from the service\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring your instances in production\n",
    "using google cloud monitoring tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "groovy"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Enable Cloud Monitoring API\n",
    "!gcloud services enable monitoring.googleapis.com\n",
    "\n",
    "# Create a monitoring dashboard\n",
    "dashboard_config = {\n",
    "    \"displayName\": \"Cloud Run GPU Utilization\",\n",
    "    \"gridLayout\": {\n",
    "        \"widgets\": [\n",
    "            {\n",
    "                \"title\": \"GPU Utilization\",\n",
    "                \"xyChart\": {\n",
    "                    \"dataSets\": [\n",
    "                        {\n",
    "                            \"timeSeriesQuery\": {\n",
    "                                \"timeSeriesFilter\": {\n",
    "                                    \"filter\": 'metric.type=\"custom.googleapis.com/gpu/utilization\" AND resource.type=\"cloud_run_revision\"',\n",
    "                                    \"aggregation\": {\n",
    "                                        \"alignmentPeriod\": \"60s\",\n",
    "                                        \"perSeriesAligner\": \"ALIGN_MEAN\"\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    ],\n",
    "                    \"timeshiftDuration\": \"0s\",\n",
    "                    \"yAxis\": {\n",
    "                        \"label\": \"Utilization\",\n",
    "                        \"scale\": \"LINEAR\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save the dashboard configuration to a file\n",
    "with open('dashboard.json', 'w') as f:\n",
    "    json.dump(dashboard_config, f)\n",
    "\n",
    "# Create the dashboard using gcloud\n",
    "!gcloud monitoring dashboards create --config-from-file=dashboard.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add GPUS to your cloud run services "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "groovy"
    }
   },
   "outputs": [],
   "source": [
    "# Update Cloud Run service to use GPUs\n",
    "!gcloud run services update distilbert-app --platform managed --region us-central1 --update-env-vars=GPU_TYPE=nvidia-tesla-t4,GPU_COUNT=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push a change to your serving application\n",
    "Realize that there is a refinement of changes that need to be made \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Flask Application\n",
    "%%writefile app.py\n",
    "from flask import Flask, request, jsonify\n",
    "from lightllm import LLMPredictor\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Initialize Flask Application\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load LightLLM Model\n",
    "predictor = LLMPredictor(model_path=\"./model\")\n",
    "\n",
    "# Define Route for Text Prediction\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    # Get JSON data from the POST request\n",
    "    data = request.get_json()\n",
    "    \n",
    "    # Extract the text input from the JSON data\n",
    "    text = data.get('text')\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    \n",
    "    # Perform inference using the model\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # Apply softmax to get probabilities\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    # Return the predictions as a JSON response\n",
    "    return jsonify(predictions.tolist())\n",
    "\n",
    "# Example usage\n",
    "text = \"Hello, how are you?\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "# Run Flask Application\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=8080)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Splitting \n",
    "cloud run allows for traffic splitting between the two versions, take advantage of this! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Cloud Run service to split traffic between revisions\n",
    "!gcloud run services update-traffic distilbert-app --platform managed --region us-central1 --to-revisions revision-1=50,revision-2=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nirvana\n",
    "welcome to paradise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Display the image\n",
    "image_url = \"https://example.com/path/to/buddha_image.jpg\"  # Replace with the actual URL of the image\n",
    "display(Image(url=image_url, width=400, height=300))\n",
    "\n",
    "# Display the text\n",
    "print(\"nirvana\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
